# 反思如何改进任务输出 Reflection to improve outputs of a task

**“反思模式”是经常使用且实现起来“出奇地简单”的一种方法。**

将这一过程比作人类写作：我们写完初稿后，会回头阅读、发现问题（如模糊不清、拼写错误、遗漏署名），然后进行修改，最终得到一份更完善的文稿。

核心类比： 人类反思 → AI反思。这不仅是模仿，更是赋予AI一种自我优化的能力。 


## 案例1：以写邮件为例：

### 人类反思流程：从初稿到定稿

人类反思流程：从初稿到定稿
Ng 自己写给 Tommy 的邮件为例，演示人类的反思过程：
1. 初稿 (Email V1):
  - 问题1：模糊性 - “next month” 不够具体，Tommy 无法确定是哪几天。
  - 问题2：拼写错误 - “fre” 应为 “free”。
  - 问题3：不完整 - 邮件没有署名。
2. 反思与改进 (Email V2):
  - 改进点： 明确了日期范围（5th-7th），修正了拼写错误，并添加了署名。


### AI反思流程：硬编码的“提示-反思”工作流

1. 第一阶段：生成初稿 (Write first draft)
  - 向 LLM 发送一个初始提示（Prompt），例如：“Write an email...”。
  - LLM 根据提示生成第一个版本的输出（Email V1）。

2. 第二阶段：反思与改进 (Reflect and write improved second draft)
  - 将第一版输出（Email V1）作为新的输入，再次发送给 LLM（可以是同一个模型，也可以是另一个专门用于推理的模型）。
  - 这次的提示语会不同，例如：“请反思这份邮件草稿，并写出一个改进版。”
  - LLM 基于对自身初稿的分析，生成最终的、质量更高的第二版输出（Email V2）。



**关键点： 这个流程是“硬编码”的，即工程师预先设计好整个步骤，而不是让模型自主决定何时反思。这是一种可靠且易于实现的工程化方法。**

![alt text](../images/2.1.1.png)


## 案例2：代码编写

1. 基础版：模型自省
  - 步骤1： 提示 LLM 编写一段代码（code V1）。
  - 步骤2： 将 code V1 再次输入给 LLM，提示其“检查代码中的错误并写出改进版”。
  - 结果： 得到修复了潜在 bug 的 code V2。

2. 进阶版：利用“思考模型” (Reasoning Models)去“检查代码中的错误并写出改进版”
  - 不同的 LLM 有不同的专长。可以使用一个擅长快速生成的模型来写初稿，再用一个“思考模型”（Thinking Model）、更擅长逻辑推理和错误排查的模型来进行反思和改进。
  - 这种组合能发挥各自的优势，达到“1+1>2”的效果。

![alt text](../images/2.1.2.png)

3. 终极形态：结合外部反馈的反思

反思最强大的形式是结合外部反馈。仅仅让模型“内省”是有限的，而引入来自模型之外的新信息，则能带来质的飞跃。

以上面的代码为例做结合外部反馈的反思：
    1. 生成初稿： LLM 生成 code V1。
    2. 执行代码： 在安全的沙盒环境中运行 code V1。
    3. 获取外部反馈： 系统捕获代码的实际输出（Output）和任何错误信息（Errors），例如 SyntaxError: unterminated string literal。
    4. 反思与改进： 将 code V1、Output 和 Errors 一起作为输入，交给 LLM 进行反思，要求其根据这些具体的、客观的反馈来重写代码。
    5. 获得终稿： LLM 基于真实的执行结果，修正错误，生成功能正确的 code V2。

![alt text](../images/2.1.3.png)

**总结：**
1. 反思不是魔法，而是工程实践： 它不能保证模型每次都100%正确，但能带来“适度的性能提升”，是性价比极高的优化手段。
2. 外部反馈是关键： 反思的力量在于能否获取并利用外部信息。如果能运行代码、查询数据库或调用API，将这些结果作为反馈输入，就能让模型进行更深层次的反思，从而产出更优的结果。
3. 设计哲学：当你有机会获取额外信息时，请务必将其融入反思流程。这是提升系统鲁棒性和输出质量的核心策略。